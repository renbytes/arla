# simulations/emergence_sim/config/emergence_config.yml

# Core Simulation Setup
simulation:
  steps: 500
  log_directory: "data/logs/emergence_voter_model"
  database_directory: "data/db"
  database_file: "agent_sim.db"
  enable_debug_logging: false
  random_seed: 42
  # All complex systems are disabled for this simple game
  systems:
    enable_symbol_negotiation: false
    enable_narrative_consensus: false
    enable_ritualization: false
    enable_social_credit: false
    enable_normative_abstraction: false

# Action and Component Definitions
action_modules:
  - "simulations.emergence_sim.actions.influence_action"

logging:
  components_to_log:
    - "agent_core.core.ecs.component.TimeBudgetComponent"
    - "simulations.emergence_sim.components.PositionComponent"
    - "simulations.emergence_sim.components.OpinionComponent"
    - "agent_core.core.ecs.component.IdentityComponent"

# LLM and Agent Definitions
llm:
  provider: "openai"
  completion_model: "gpt-4o-mini"
  embedding_model: "text-embedding-3-small"
  temperature: 0.0
  max_tokens: 500
  reflection_prompt_prefix: "You are an agent in a simulation. Reflect on your recent experiences. In one paragraph, what have you learned about your group and its beliefs?"

agent:
  foundational:
    num_agents: 40
    lifespan_std_dev_percent: 0.0
    vitals:
      initial_time_budget: 1000.0
      initial_health: 100.0
      initial_resources: 0.0 # Resources not needed for this model
    attributes:
      initial_attack_power: 0.0
      initial_speed: 1

  cognitive:
    embeddings:
      main_embedding_dim: 1536
      schema_embedding_dim: 128

  costs:
    actions:
      influence: 1.0 # Cost for the only action
      # Add missing fields required by the schema with default values
      propose_symbol: 1.0
      guess_object: 1.0
      share_narrative: 1.0
      give_resource: 1.0
      request_resource: 1.0


  emotional_dynamics:
    temporal:
      valence_decay_rate: 0.95
      arousal_decay_rate: 0.90
      valence_learning_rate: 0.2
      arousal_learning_rate: 0.3
    noise_std: 0.02
    appraisal_weights:
      goal_relevance: 0.7
      agency: 0.5
      social_feedback: 0.3

  dynamics:
    decay:
      # No resource decay needed
      resources_per_step: 0.0

# Environment and Learning
environment:
  grid_world_size: [10, 10]
  # No objects are needed for this simple model
  num_objects: 0

learning:
  q_learning:
    initial_epsilon: 0.1 # Low exploration, as there's only one action
    epsilon_decay_rate: 0.999
    min_epsilon: 0.01
    alpha: 0.001
    gamma: 0.99
    state_feature_dim: 16
    internal_state_dim: 8
    action_feature_dim: 15 # Kept for compatibility, though less critical now
  memory:
    short_term_memory_maxlen: 10
    affective_buffer_maxlen: 200
    emotion_cluster_min_data: 50
    reflection_interval: 50 # Reflect more often on opinion changes
    cognitive_dissonance_threshold: 0.7
  rewards:
    base_reward: 0.0
    # Add missing fields required by the schema with default values
    move_reward_base: 0.0
    give_resource_bonus: 0.0